# Machine Learning Model Deployment Guide

## Table of Contents
- [Overview](#overview)
- [Model Deployment Workflow](#model-deployment-workflow)
- [Environment and Dependency Management](#environment-and-dependency-management)
- [Containerization with Docker](#containerization-with-docker)
- [Cloud Deployment](#cloud-deployment)
- [FAQ](#faq)
- [Best Practices](#best-practices)
- [Advanced Topics](#advanced-topics)

---

## Overview

Machine Learning Model Deployment is the process of making a trained ML model available as a web service that can be accessed by other applications in real-time. This guide covers the complete workflow from model preparation to cloud deployment.

### Why Deploy ML Models?

- **Real-time Predictions**: Enable other services to get instant predictions
- **Scalability**: Handle multiple requests simultaneously
- **Integration**: Allow different applications to use your model
- **Production Use**: Move from experimentation to business value

---

## Model Deployment Workflow

### 1. Model Preparation and Saving

The first step involves preparing your trained model for deployment:

#### Saving with Pickle
```python
import pickle

# Save model and preprocessor
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('preprocessor.pkl', 'wb') as f:
    pickle.dump(dict_vectorizer, f)
```

**Key Components to Save:**
- **Trained Model**: The actual ML algorithm (e.g., LogisticRegression)
- **Feature Transformers**: Preprocessors like DictVectorizer
- **Any preprocessing steps**: Scalers, encoders, etc.

**Critical Requirements:**
- âš ï¸ **Exact same dependencies** must be installed in production
- âš ï¸ **Same library versions** to avoid compatibility issues
- âš ï¸ **Consistent Python environment**

### 2. Creating a Web Service

Transform your model into a network-accessible service using **Flask**:

```python
from flask import Flask, request, jsonify
import pickle

app = Flask(__name__)

# Load model and preprocessor
with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

with open('preprocessor.pkl', 'rb') as f:
    dv = pickle.load(f)

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    
    # Transform data
    X = dv.transform([data])
    
    # Make prediction
    prediction = model.predict_proba(X)[0, 1]
    
    return jsonify({'probability': float(prediction)})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=9696)
```

**Benefits of Flask:**
- ğŸš€ Simple to implement
- ğŸŒ HTTP protocol support (GET, POST)
- ğŸ”§ Easy to test and debug
- ğŸ“¡ Network accessibility

---

## Environment and Dependency Management

### Python Dependency Isolation with Pipenv

**The Problem:** Different projects need different library versions

**The Solution:** Virtual environments with Pipenv

#### Setup Pipenv Environment:
```bash
# Install pipenv
pip install pipenv

# Create virtual environment and install dependencies
pipenv install flask scikit-learn pandas numpy

# Install development dependencies
pipenv install --dev pytest

# Activate environment
pipenv shell

# Run application
pipenv run python app.py
```

#### Key Pipenv Files:
- **`Pipfile`**: Human-readable dependency specification
- **`Pipfile.lock`**: Exact versions and checksums for reproducibility

**Example Pipfile:**
```toml
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
flask = "*"
scikit-learn = "==1.6.1"
pandas = "*"

[dev-packages]
pytest = "*"

[requires]
python_version = "3.12"
```

**Benefits:**
âœ… **Isolation**: Separate dependencies per project  
âœ… **Reproducibility**: Locked versions ensure consistency  
âœ… **Security**: Checksums verify package integrity  
âœ… **Simplicity**: Easy to manage and share  

---

## Containerization with Docker

### Why Docker?

Virtual environments handle Python dependencies, but Docker provides **complete system isolation**:

- ğŸ–¥ï¸ **Operating System**: Specific Linux distributions
- ğŸ”§ **System Libraries**: Compilers, system tools
- ğŸ“¦ **Complete Application**: Everything needed to run
- ğŸŒ **Portability**: Runs anywhere Docker is supported

### Creating a Dockerfile

```dockerfile
FROM python:3.12-slim

WORKDIR /app

# Copy dependency files
COPY Pipfile Pipfile.lock ./

# Install pipenv and dependencies
RUN pip install pipenv && \
    pipenv install --system --deploy

# Copy application code
COPY app.py model.pkl preprocessor.pkl ./

# Expose port
EXPOSE 9696

# Run application
ENTRYPOINT ["python", "app.py"]
```

### Docker Commands

```bash
# Build image
docker build -t ml-model-service .

# Run container
docker run -p 9696:9696 ml-model-service

# Test the service
curl -X POST http://localhost:9696/predict \
     -H "Content-Type: application/json" \
     -d '{"feature1": "value1", "feature2": 123}'
```

**Benefits:**
âœ… **Complete Isolation**: OS-level separation  
âœ… **Consistency**: Same environment everywhere  
âœ… **Portability**: Deploy anywhere  
âœ… **Scalability**: Easy to replicate  

---

## Cloud Deployment

### AWS Elastic Beanstalk (EB)

**What is Elastic Beanstalk?**
A Platform-as-a-Service (PaaS) that simplifies deployment by automatically handling:

- ğŸ—ï¸ **Infrastructure Setup**: Load balancers, auto-scaling groups
- ğŸ“ˆ **Auto-scaling**: Automatically adjust capacity based on demand
- ğŸ”§ **Configuration Management**: Environment variables, logging
- ğŸ”’ **Security**: Basic security configurations

#### Deployment Steps:

```bash
# Install EB CLI
pip install awsebcli

# Initialize EB application
eb init

# Create environment and deploy
eb create ml-model-production

# Check status
eb status

# View logs
eb logs

# Terminate when done (important for cost!)
eb terminate
```

#### EB Configuration Example:

**`.ebextensions/01_python.config`:**
```yaml
option_settings:
  aws:elasticbeanstalk:container:python:
    WSGIPath: app:app
  aws:elasticbeanstalk:application:environment:
    PYTHONPATH: "/var/app/current"
```

#### Auto-scaling Configuration:
```yaml
option_settings:
  aws:autoscaling:asg:
    MinSize: 1
    MaxSize: 10
  aws:autoscaling:trigger:
    MeasureName: CPUUtilization
    Unit: Percent
    UpperThreshold: 70
    LowerThreshold: 30
```

**Benefits:**
âœ… **Automated Infrastructure**: No manual server setup  
âœ… **Auto-scaling**: Handle traffic spikes automatically  
âœ… **Load Balancing**: Distribute requests across instances  
âœ… **Monitoring**: Built-in health checks and metrics  

---

## FAQ

### Q1: Why is Flask used in this deployment context?

**Answer**: Flask is a lightweight Python web framework that makes it easy to:
- Convert Python functions into HTTP endpoints
- Handle web requests (GET, POST) and responses
- Create RESTful APIs for model predictions
- Integrate with existing Python ML pipelines
- Provide a simple interface for other services to consume

### Q2: What essential components are saved alongside the model using pickle?

**Answer**: You must save all components needed for the complete prediction pipeline:
- **Trained Model**: The ML algorithm (e.g., LogisticRegression)
- **Feature Transformers**: DictVectorizer, StandardScaler, etc.
- **Preprocessing Steps**: Any data cleaning or transformation logic
- **Label Encoders**: For categorical target variables
- **Feature Selection**: If you applied feature selection

**Example:**
```python
# Save complete pipeline
pipeline_components = {
    'model': trained_model,
    'vectorizer': dict_vectorizer,
    'scaler': standard_scaler
}
pickle.dump(pipeline_components, open('pipeline.pkl', 'wb'))
```

### Q3: How does Pipenv ensure dependency consistency?

**Answer**: Pipenv uses a two-file system:
- **`Pipfile`**: Human-readable, specifies dependency ranges
- **`Pipfile.lock`**: Machine-generated, contains exact versions and checksums

This ensures:
- ğŸ”’ **Exact Versions**: No "works on my machine" issues
- ğŸ›¡ï¸ **Security**: Checksums verify package integrity
- ğŸ”„ **Reproducibility**: Same environment across development, testing, and production
- ğŸ“‹ **Transitive Dependencies**: All sub-dependencies are locked

### Q4: What is Docker's primary role in deployment?

**Answer**: Docker provides **complete system isolation** beyond what virtual environments offer:

| Virtual Environment | Docker Container |
|-------------------|------------------|
| Python packages only | Complete OS + packages |
| Shared system libraries | Isolated system libraries |
| Host OS dependent | OS independent |
| Library conflicts possible | Complete isolation |

**Docker Benefits:**
- ğŸ“¦ **Packaging**: Everything in one container
- ğŸŒ **Portability**: Run anywhere Docker runs
- ğŸ”§ **Consistency**: Same environment across all stages
- ğŸš€ **Deployment**: Easy to deploy to any cloud platform

### Q5: What are the key functions of AWS Elastic Beanstalk?

**Answer**: EB automates the complex parts of deployment:

**Infrastructure Management:**
- ğŸ—ï¸ **Load Balancers**: Distribute traffic across instances
- ğŸ“ˆ **Auto-scaling Groups**: Scale up/down based on demand
- ğŸ–¥ï¸ **EC2 Instances**: Manage compute resources
- ğŸŒ **Networking**: VPC, security groups, subnets

**Operational Features:**
- ğŸ“Š **Monitoring**: CloudWatch metrics and alarms
- ğŸ“‹ **Logging**: Centralized log collection
- ğŸ”„ **Rolling Deployments**: Zero-downtime updates
- ğŸ’¾ **Configuration Management**: Environment variables and settings

### Q6: What is the simplest way to deploy using EB CLI?

**Answer**: Three core commands:

```bash
# 1. Initialize (one-time setup)
eb init my-ml-app
# Choose region, platform (Docker), and other settings

# 2. Create and deploy
eb create production-env
# Creates environment and deploys application

# 3. Terminate (to avoid charges)
eb terminate production-env
# Cleans up all resources
```

**Additional Useful Commands:**
```bash
eb deploy          # Deploy new version
eb status          # Check environment health
eb logs            # View application logs
eb open            # Open app in browser
eb config          # Modify configuration
```

### Q7: Why must deployed services be secured?

**Answer**: Default deployments are often **open to the world** (0.0.0.0/0), which means:

**Security Risks:**
- ğŸš¨ **Unauthorized Access**: Anyone can use your model
- ğŸ’° **Cost**: Unwanted traffic increases charges
- ğŸ“Š **Data Exposure**: Potential information leakage
- ğŸ› **DDoS**: Service can be overwhelmed

**Security Measures:**
```yaml
# Security Group Rules (restrict access)
option_settings:
  aws:ec2:vpc:
    ELBSubnets: subnet-12345, subnet-67890
    VPCId: vpc-abcdef123
  aws:elasticbeanstalk:environment:
    LoadBalancerType: application
```

**Best Practices:**
- ğŸ” **API Keys**: Require authentication
- ğŸŒ **IP Whitelisting**: Allow only trusted sources
- ğŸ”’ **HTTPS**: Encrypt data in transit
- ğŸ“ **Rate Limiting**: Prevent abuse

---

## Best Practices

### 1. Model Versioning
```python
# Include version in model filename
model_version = "v1.2.3"
pickle.dump(model, open(f'model_{model_version}.pkl', 'wb'))
```

### 2. Health Check Endpoint
```python
@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'version': '1.0.0'})
```

### 3. Error Handling
```python
@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        prediction = model.predict_proba([data])[0, 1]
        return jsonify({'prediction': float(prediction)})
    except Exception as e:
        return jsonify({'error': str(e)}), 400
```

### 4. Logging
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.route('/predict', methods=['POST'])
def predict():
    logger.info(f"Received prediction request: {request.get_json()}")
    # ... prediction logic ...
    logger.info(f"Returning prediction: {prediction}")
```

### 5. Configuration Management
```python
import os

class Config:
    MODEL_PATH = os.environ.get('MODEL_PATH', 'model.pkl')
    DEBUG = os.environ.get('DEBUG', 'False').lower() == 'true'
    PORT = int(os.environ.get('PORT', 9696))
```

---

## Advanced Topics

### 1. Model Monitoring and Observability

Once deployed, monitoring is crucial for production systems:

**Infrastructure Metrics:**
- ğŸ“Š **Latency**: Response time monitoring
- ğŸ–¥ï¸ **Resource Utilization**: CPU, memory, disk usage
- ğŸ“ˆ **Scaling Events**: Auto-scaling activity
- ğŸ”— **Request Volume**: Traffic patterns

**Model Performance Metrics:**
- ğŸ“‰ **Data Drift**: Changes in input data characteristics
- ğŸ¯ **Model Drift**: Decrease in predictive accuracy over time
- ğŸ“Š **Prediction Distribution**: Output pattern changes
- ğŸ”„ **Feedback Loops**: Actual vs predicted outcomes

**Implementation Example:**
```python
from prometheus_client import Counter, Histogram, generate_latest

REQUEST_COUNT = Counter('ml_requests_total', 'Total ML requests')
REQUEST_LATENCY = Histogram('ml_request_duration_seconds', 'Request latency')

@app.route('/predict', methods=['POST'])
def predict():
    REQUEST_COUNT.inc()
    with REQUEST_LATENCY.time():
        # prediction logic
        pass
```

### 2. API Gateway Management

For production systems, place an API Gateway in front of your service:

**API Gateway Functions:**
- ğŸ” **Authentication**: API key enforcement
- ğŸš¦ **Rate Limiting**: Prevent abuse
- ğŸ”’ **SSL/TLS Termination**: Secure connections
- ğŸŒ **Traffic Routing**: Load balancing and routing
- ğŸ“Š **Analytics**: Request tracking and monitoring

**AWS API Gateway Example:**
```yaml
# serverless.yml
service: ml-model-api

provider:
  name: aws
  runtime: python3.12

functions:
  predict:
    handler: app.predict
    events:
      - http:
          path: predict
          method: post
          cors: true
```

### 3. Alternative Serialization Formats

**Limitations of Pickle:**
- ğŸš¨ **Security Risk**: Can execute arbitrary code
- ğŸ **Python-only**: Restricts to Python environments
- ğŸ“¦ **Library Dependencies**: Requires exact same libraries

**Alternatives:**

#### ONNX (Open Neural Network Exchange)
```python
# Convert scikit-learn to ONNX
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]
onnx_model = convert_sklearn(model, initial_types=initial_type)

# Save ONNX model
with open("model.onnx", "wb") as f:
    f.write(onnx_model.SerializeToString())
```

#### MLflow Model Format
```python
import mlflow.sklearn

# Save model in MLflow format
mlflow.sklearn.save_model(model, "model_mlflow")

# Load model
loaded_model = mlflow.sklearn.load_model("model_mlflow")
```

**Benefits:**
âœ… **Language Agnostic**: Use from any programming language  
âœ… **Security**: No arbitrary code execution  
âœ… **Standardization**: Industry-standard formats  
âœ… **Performance**: Optimized for inference  

### 4. Container Orchestration

For large-scale deployments, consider container orchestration:

**Kubernetes Deployment:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model
  template:
    metadata:
      labels:
        app: ml-model
    spec:
      containers:
      - name: ml-model
        image: ml-model-service:latest
        ports:
        - containerPort: 9696
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

### 5. CI/CD Pipeline

Automate deployment with continuous integration:

**GitHub Actions Example:**
```yaml
name: Deploy ML Model

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Build Docker image
      run: docker build -t ml-model .
    
    - name: Deploy to staging
      run: |
        eb init --platform docker
        eb create staging-env
    
    - name: Run tests
      run: python -m pytest tests/
    
    - name: Deploy to production
      if: success()
      run: eb deploy production-env
```

---

## Key Takeaways ğŸ¯

1. **Start Simple**: Begin with Flask and Pipenv
2. **Containerize**: Use Docker for consistency and portability
3. **Automate**: Leverage cloud platforms like AWS EB
4. **Monitor**: Implement comprehensive monitoring from day one
5. **Secure**: Never deploy without proper security measures
6. **Version**: Always version your models and APIs
7. **Test**: Implement automated testing for your deployment pipeline

Remember: **The goal is not just to deploy a model, but to deploy a reliable, scalable, and maintainable ML service that provides business value!** ğŸš€

---

*Created with â¤ï¸ for ML Zoomcamp 2025 - Week 5: Deployment*
